# Association Rules -------------------------------------------------------
# arules and arulesViz packages install
install.packages("arules")
install.packages("arulesViz")
install.packages("wordcloud")
library(arules)
library(arulesViz)
library(wordcloud)
# Part 1: Transform a data file into transaction format
# Basket type
tmp_basket <- read.transactions("Transaction_Sample_Basket.csv",
format = "basket", sep = ",", rm.duplicates=TRUE)
inspect(tmp_basket)
# Single type
tmp_single <- read.transactions("Transaction_Sample_Single.csv",
format = "single", cols = c(1,2), rm.duplicates=TRUE)
inspect(tmp_single)
# Part 2: Association Rule Mining without sequence information
data("Groceries")
summary(Groceries)
str(Groceries)
itemInfo(Groceries)
groceries_df <- as(Groceries, "data.frame")
# Item inspection
itemName <- itemLabels(Groceries)
itemCount <- itemFrequency(Groceries)*nrow(Groceries)
col <- brewer.pal(8, "Dark2")
wordcloud(words = itemName, freq = itemCount, min.freq = 1, scale = c(3, 0.2), col = col , random.order = FALSE)
itemFrequencyPlot(Groceries, support = 0.05, cex.names=0.8)
# Rule generation by Apriori
rules <- apriori(Groceries, parameter=list(support=0.01, confidence=0.35))
# Check the generated rules
inspect(rules)
# List the first three rules with the highest lift values
inspect(sort(rules, by="lift"))
# Save the rules in a text file
write.csv(as(rules, "data.frame"), "Groceries_rules.csv", row.names = FALSE)
# Plot the rules
plot(rules, method = "scatterplot")
plotly_arules(rules, method = "scatterplot", measure = c("support", "confidence"), shading = "lift")
plot(rules, method="matrix")
plotly_arules(rules, method = "matrix", measure = c("support", "confidence"), shading = "lift")
# Rule generation by Apriori with another parameters
rules <- apriori(Groceries, parameter=list(support=0.01, confidence=0.5))
plot(rules, method="graph")
plot(rules, method="paracoord")
setwd("~/")
library(Ecdat)
data(Cigar)
Cigar <- Cigar[Cigar$year %in% c(91,92),]
summary(Cigar)
unique(Cigar$year)
sum(Cigar$year==unique(Cigar))
y <- log(Cigar$sales)[Cigar$year==unique(Cigar$year)]
ybar <- mean(y)
x <- log(Cigar$price)[Cigar$year==unique(Cigar$year)]
xbar <- mean(x)
SSTx <- sum((x-xbar)^2)
Sxy <- sum((x-xbar)*y)
Sxyb <- sum(x*(y-ybar))
Sxyc <- sum((x-xbar)*(y-ybar))
Sxyd <- sum((x-xbar)*(y-100))
b1hat <- Sxy/SSTx
lm(log(sales)~log(price),data=Cigar, subset = year == unique(Cigar$year))
ols <- lm(log(sales)~log(price),data=Cigar, subset = year == unique(Cigar$year))
yhat <- ols$fitted.values
uhat <- ols$residuals
mean(yhat)
SST <- sum((y-ybar)^2)
SSE <- sum((yhat-ybar)^2)
SSR <- sum(uhat^2)
Rsq <- SSE/SST
summary(ols)
ybar
SSTx
Sxy
Sxyb
b1hat
yhat
mean(yhat)
SST
SSE
SSR
Rsq
summary(ols)
rm(list=ls(all=TRUE))
library(ISLR)
library(ggplot2)
library(gplots)
library(pheatmap)
library(RColorBrewer)
library(clValid)
library(plotrix)
library(cluster)
library(factoextra)
library(dbscan)
#[Extra Question]
data("USArrests")
head(USArrests)
res.fanny <- fanny(df, 2)
df <- scale(USArrests)
res.fanny <- fanny(df, 2)
head(res.fanny$clustering)
fviz_cluster(res.fanny, ellipse.type = "norm", repel = TRUE,
palette = "jco", ggtheme = theme_minimal(),
legend = "right")
head(USArrests)
help("USArrests")
View(USArrests)
USArrests
res.fanny <- fanny(df, 2)
head(res.fanny$clustering)
fviz_cluster(res.fanny, ellipse.type = "norm", repel = TRUE,
palette = "jco", ggtheme = theme_minimal(),
legend = "right")
fviz_silhouette(res.fanny, palette = "jco",
ggtheme = theme_minimal())
res.fanny <- fanny(df, 3)
fviz_cluster(res.fanny, ellipse.type = "norm", repel = TRUE,
palette = "jco", ggtheme = theme_minimal(),
legend = "right")
fviz_silhouette(res.fanny, palette = "jco", ggtheme = theme_minimal())
res.fanny <- fanny(df, 2)
fviz_silhouette(res.fanny, palette = "jco", ggtheme = theme_minimal())
setwd("C:/Users/hebte/Desktop/Kaggle/Credit card fraud")
credit <- read.csv("creditcard.csv")
# Performance Evaluation Function
perf_eval2 <- function(cm){
# True positive rate: TPR (Recall)
TPR <- cm[2,2]/sum(cm[2,])
# Precision
PRE <- cm[2,2]/sum(cm[,2])
# True negative rate: TNR
TNR <- cm[1,1]/sum(cm[1,])
# Simple Accuracy
ACC <- (cm[1,1]+cm[2,2])/sum(cm)
# Balanced Correction Rate
BCR <- sqrt(TPR*TNR)
# F1-Measure
F1 <- 2*TPR*PRE/(TPR+PRE)
return(c(TPR, PRE, TNR, ACC, BCR, F1))
}
# Initialize the performance matrix
perf_mat <- matrix(0, 1, 6)
colnames(perf_mat) <- c("TPR (Recall)", "Precision", "TNR", "ACC", "BCR", "F1")
rownames(perf_mat) <- "Logstic Regression"
View(credit)
input_idx <- c(1:30)
target_idx <- 31
input_idx <- c(1:30)
target_idx <- 31
Credit_input <- Credit[,input_idx]
Credit_target <- Credit[,target_idx]
input_idx <- c(1:30)
target_idx <- 31
credit_input <- credit[,input_idx]
credit_target <- credit[,target_idx]
set.seed(12345)
trn_idx <- sample(1:nrow(credit), round(0.7*nrow(credit)))
credit_trn <- credit[trn_idx,]
credit_tst <- credit[-trn_idx,]
full_lr <- glm(Class ~ ., family=binomial, credit_trn)
summary(full_lr)
lr_response <- predict(full_lr, type = "response", newdata = credit_tst)
lr_target <- credit_tst$Class
response_table <- data.frame(lr_response,lr_target)
response_table
lr_predicted1 <- rep(0, length(lr_target))
lr_predicted1[which(lr_response >= 0.5)] <- 1
cm_full1 <- table(lr_target, lr_predicted1)
cm_full1
perf_mat[1,] <- perf_eval2(cm_full1)
perf_mat
lr_predicted1 <- rep(0, length(lr_target))
lr_predicted1[which(lr_response >= 0.3)] <- 1
cm_full1 <- table(lr_target, lr_predicted1)
cm_full1
perf_mat[1,] <- perf_eval2(cm_full1)
perf_mat
ROC <- function(prob_table){
prob_table <- prob_table[order(-prob_table[,1]),]
ROC_TPR <- rep(0,nrow(prob_table))
ROC_FPR <- rep(0,nrow(prob_table))
ROC_actualNG <- length(which(prob_table[,2]==1))
ROC_actualG <- length(which(prob_table[,2]==0))
NG_count <- 0
G_count <- 0
ROC_table <- data.frame(prob_table, ROC_TPR, ROC_FPR)
for(i in (1:nrow(ROC_table))){
if(ROC_table[i,2]==1){
NG_count = NG_count + 1
ROC_table[i,3] = NG_count/ROC_actualNG
}
else
NG_count = NG_count
ROC_table[i,3] = NG_count/ROC_actualNG
}
for(i in (1:nrow(ROC_table))){
if(ROC_table[i,2]==0){
G_count = G_count + 1
ROC_table[i,4] = G_count/ROC_actualG
}
else
G_count = G_count
ROC_table[i,4] = G_count/ROC_actualG
}
return(ROC_table)
}
AUROC <-function(roc_table){
ROC_actualG <- length(which(roc_table[,2]==0))
AUROC <- 0
for(i in (1:(nrow(roc_table)-1)))
if(roc_table[i,4]!=roc_table[i+1,4])
AUROC = AUROC + roc_table[i,3]*(1/ROC_actualG)
else
AUROC = AUROC
return(AUROC)
}
roc_table <- ROC(response_table)
AUROC(roc_table)
summary(full_lr)
cm_full1
perf_mat
roc_table <- ROC(response_table)
AUROC(roc_table)
ggplot(data=roc_table,aes(x=roc_table[,4],y=roc_table[,3]))+geom_path()+
labs(x="1-specificity",y="sensitivity",title="ROC curve of count")
library(ggplot2)
ggplot(data=roc_table,aes(x=roc_table[,4],y=roc_table[,3]))+geom_path()+
labs(x="1-specificity",y="sensitivity",title="ROC curve of count")
